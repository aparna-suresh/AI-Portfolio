{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:48:42.787516Z",
     "start_time": "2025-12-10T16:47:20.896604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig, OptimizationConfig\n",
    "from sympy.printing.pytorch import torch\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModel, ORTQuantizer, ORTOptimizer\n",
    "from pyarrow import parquet"
   ],
   "id": "47ea77eca8ad9151",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:48:55.949406Z",
     "start_time": "2025-12-10T16:48:53.353996Z"
    }
   },
   "cell_type": "code",
   "source": "df_filtered = pd.read_csv(\"data/amazon_review_xgboost.csv\")",
   "id": "c9b2e959570f2026",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:48:59.319988Z",
     "start_time": "2025-12-10T16:48:56.806514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "dataset_with_rating_bins = Dataset.from_pandas(df_filtered, preserve_index=False)\n",
    "dataset_with_rating_bins"
   ],
   "id": "c7b34f0cad934be0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase', 'embedding_text', 'review_word_count', 'title_word_count', '__index_level_0__', 'rating_bins', 'total_review_word_count'],\n",
       "    num_rows: 636206\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:21:04.840876Z",
     "start_time": "2025-12-10T05:21:04.830345Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_with_rating_bins.features",
   "id": "ab7f24f1eceffaf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rating': Value(dtype='float64', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'images': Value(dtype='string', id=None),\n",
       " 'asin': Value(dtype='string', id=None),\n",
       " 'parent_asin': Value(dtype='string', id=None),\n",
       " 'user_id': Value(dtype='string', id=None),\n",
       " 'timestamp': Value(dtype='int64', id=None),\n",
       " 'helpful_vote': Value(dtype='int64', id=None),\n",
       " 'verified_purchase': Value(dtype='bool', id=None),\n",
       " 'embedding_text': Value(dtype='string', id=None),\n",
       " 'review_word_count': Value(dtype='int64', id=None),\n",
       " 'title_word_count': Value(dtype='int64', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None),\n",
       " 'rating_bins': Value(dtype='int64', id=None),\n",
       " 'total_review_word_count': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#region\n",
    "split_dataset = dataset_with_rating_bins.train_test_split(test_size=0.2, shuffle=True, stratify_by_column=\"rating_bins\")\n",
    "train_set = split_dataset[\"train\"],\n",
    "train_set = train_set[0]\n",
    "test_validation_split = split_dataset[\"test\"].train_test_split(test_size=0.5, shuffle=True,\n",
    "                                                               stratify_by_column=\"rating_bins\")\n",
    "validation_set = test_validation_split[\"train\"]\n",
    "test_set = test_validation_split[\"test\"]\n",
    "print(train_set)\n",
    "print(test_set)\n",
    "print(validation_set)\n",
    "# endregion"
   ],
   "id": "ad4f5ff47a71f3b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:49:14.104935Z",
     "start_time": "2025-12-10T16:49:12.467899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")\n",
    "chkpt = \"google-bert/bert-base-uncased\"\n",
    "onx_model_path = \"onx_model/bert-base-uncased/3/\""
   ],
   "id": "7f368268791c90ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T00:28:32.464411Z",
     "start_time": "2026-02-10T00:28:32.075703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert the model to ONNX, the result is model.onnx\n",
    "ort_model = ORTModel.from_pretrained(chkpt, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(chkpt)\n",
    "ort_model.save_pretrained(onx_model_path)\n",
    "tokenizer.save_pretrained(onx_model_path)\n",
    "\n",
    "# quantize the ONNX model, the result is model_quantized.onnx\n",
    "qconfig = AutoQuantizationConfig.avx2(is_static=False, per_channel=False)\n",
    "quantizer = ORTQuantizer.from_pretrained(onx_model_path, file_name=\"model.onnx\")\n",
    "# Apply dynamic quantization on the model\n",
    "quantizer.quantize(save_dir=onx_model_path, quantization_config=qconfig)\n",
    "\n",
    "optim_config = OptimizationConfig(optimization_level=1)\n",
    "optimizer = ORTOptimizer.from_pretrained(onx_model_path, file_names=[\"model_quantized.onnx\"])\n",
    "optimizer.optimize(save_dir=onx_model_path, optimization_config=optim_config)"
   ],
   "id": "cf241bfdade96b4c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ORTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# convert the model to ONNX, the result is model.onnx\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m ort_model \u001B[38;5;241m=\u001B[39m \u001B[43mORTModel\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(chkpt, export\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(chkpt)\n\u001B[0;32m      4\u001B[0m ort_model\u001B[38;5;241m.\u001B[39msave_pretrained(onx_model_path)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'ORTModel' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:50:30.889051Z",
     "start_time": "2025-12-10T16:49:18.711036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the saved tokenizer and tokenize the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(onx_model_path)\n",
    "def tokenize_text(batch):\n",
    "    text = batch[\"embedding_text\"]\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=400)\n",
    "dataset_tokenized = dataset_with_rating_bins.map(tokenize_text, batched=True)\n",
    "# print(dataset_tokenized.features)\n",
    "req_cols = ['input_ids', 'token_type_ids', 'attention_mask', 'rating']\n",
    "removed_cols = [col for col in dataset_tokenized.column_names if col not in req_cols]\n",
    "dataset_tokenized = dataset_tokenized.remove_columns(removed_cols)\n",
    "dataset_tokenized.features"
   ],
   "id": "3ffa73bf1e773c11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/636206 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71174cabb532484195d204172f7d7a0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rating': Value(dtype='float64', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T04:25:27.077410Z",
     "start_time": "2025-12-10T04:25:19.890754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_tokenized.set_format(\"arrow\")\n",
    "arrow_table = dataset_tokenized[:]\n",
    "parquet.write_table(\n",
    "    arrow_table,\n",
    "    \"data_tokenized.parquet\",\n",
    "    compression='snappy'\n",
    ")"
   ],
   "id": "5384ec74a79bc059",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:50:41.395146Z",
     "start_time": "2025-12-10T16:50:39.138764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onnxruntime\n",
    "session = onnxruntime.InferenceSession(\n",
    "    \"onx_model/bert-base-uncased/3/model_quantized.onnx\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")"
   ],
   "id": "33ebdcf2fc4b0567",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T00:28:49.990578Z",
     "start_time": "2026-02-10T00:28:49.947374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_embdeddings(batch):\n",
    "    inputs = {k: v for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    output_names = [output.name for output in session.get_outputs()]\n",
    "    ort_outputs = session.run(\n",
    "        output_names,\n",
    "        inputs  \n",
    "    )\n",
    "    # The output is typically a list, where the first element is the hidden states\n",
    "    last_hidden_state = ort_outputs[0]\n",
    "    cls_embeddings = last_hidden_state[:, 0, :]\n",
    "    return {\"cls_embeddings\": cls_embeddings}\n",
    "dataset_tokenized.set_format(type='numpy', columns=['input_ids', 'attention_mask', 'token_type_ids'])\n",
    "cls_hidden_state = dataset_tokenized.map(get_embdeddings, batched=True, batch_size=16)\n",
    "print(cls_hidden_state)"
   ],
   "id": "2e867c28ff997da7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m     cls_embeddings \u001B[38;5;241m=\u001B[39m last_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcls_embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m: cls_embeddings}\n\u001B[1;32m---> 12\u001B[0m \u001B[43mdataset_tokenized\u001B[49m\u001B[38;5;241m.\u001B[39mset_format(\u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m'\u001B[39m, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     13\u001B[0m cls_hidden_state \u001B[38;5;241m=\u001B[39m dataset_tokenized\u001B[38;5;241m.\u001B[39mmap(get_embdeddings, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(cls_hidden_state)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset_tokenized' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cls_hidden_state.set_format(\"numpy\", columns=[\"cls_embeddings\", \"rating\"])\n",
    "expanded_array = np.expand_dims(cls_hidden_state[\"rating\"], axis=0)\n",
    "expanded_array.shape"
   ],
   "id": "36212546b13df155"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stacked_embedding_rating = np.hstack((cls_hidden_state[\"cls_embeddings\"], expanded_array.T))\n",
    "stacked_embedding_rating.shape"
   ],
   "id": "ad7213efaf70b85e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "np.save('data/bert_embeddings_ratings.npy', stacked_embedding_rating)\n",
    "np.save('data/bert_only_embeddings.npy', cls_hidden_state[\"cls_embeddings\"])"
   ],
   "id": "63edb3b5c7b02251"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cls_hidden_state[\"cls_embeddings\"], cls_hidden_state[\"rating\"],random_state=42, test_size=0.2, shuffle=True,\n",
    "                                                    stratify=cls_hidden_state[\"rating\"])\n",
    "\n",
    "feature_cols = [f\"emb_{i}\" for i in range(x_train.shape[1])]\n",
    "x_train_df = pd.DataFrame(data=x_train, columns=feature_cols)\n",
    "y_train_df = pd.DataFrame(data=y_train, columns=[\"rating\"])\n",
    "train_df_combined = pd.concat([x_train_df, y_train_df], axis=1)\n",
    "x_test_df = pd.DataFrame(data=x_test, columns=feature_cols)\n",
    "y_test_df = pd.DataFrame(data=y_test, columns=[\"rating\"])\n",
    "test_df_combined = pd.concat([x_test_df, y_test_df], axis=1)\n",
    "train_df_combined.to_parquet('train_data.parquet', index=False)\n",
    "test_df_combined.to_parquet('val_test_data.parquet', index=False)"
   ],
   "id": "d7ea97300b533e22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_x, test_x, val_y, test_y = train_test_split(x_test, y_test, random_state=42, test_size=0.1, shuffle=True, stratify=y_test)\n",
    "val_y.shape"
   ],
   "id": "e73a1bfa30b2ffc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_x_df = pd.DataFrame(data=val_x, columns=feature_cols)\n",
    "val_y_df = pd.DataFrame(data=val_y, columns=[\"rating\"])\n",
    "val_df_combined = pd.concat([val_x_df, val_y_df], axis=1)\n",
    "val_df_combined.to_parquet('validation_data.parquet', index=False)\n",
    "test_x_df = pd.DataFrame(data=test_x, columns=feature_cols)\n",
    "test_y_df = pd.DataFrame(data=test_y, columns=[\"rating\"])\n",
    "test_df_combined = pd.concat([test_x_df, test_y_df], axis=1)\n",
    "test_df_combined.to_parquet('test_data.parquet', index=False)"
   ],
   "id": "59a109f87ebab832"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
