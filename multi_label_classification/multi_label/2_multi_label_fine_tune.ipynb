{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-08T00:04:53.688414Z",
     "start_time": "2026-02-08T00:03:44.884972Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import mlflow\n",
    "from hyperopt import hp\n",
    "from hyperopt import Trials, fmin, tpe, STATUS_OK\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score,\n",
    "                             accuracy_score, hamming_loss, jaccard_score,multilabel_confusion_matrix,ConfusionMatrixDisplay)\n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aiML\\NLP\\llms\\multi_label_classification\\multi_label\\.venv\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:02.878201Z",
     "start_time": "2026-02-08T00:25:02.869091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")"
   ],
   "id": "d907a70de4daa4a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:05.316256Z",
     "start_time": "2026-02-08T00:25:04.436690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_name = \"Multi-label classification\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "id": "de4a248b109ad80d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 16:25:04 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/02/07 16:25:04 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/02/07 16:25:04 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/02/07 16:25:04 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/02/07 16:25:04 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/02/07 16:25:04 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/02/07 16:25:05 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/07 16:25:05 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///D:/aiML/NLP/llms/multi_label_classification/multi_label/mlruns/1', creation_time=1770319170928, experiment_id='1', last_update_time=1770319170928, lifecycle_stage='active', name='Multi-label classification', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:07.992062Z",
     "start_time": "2026-02-08T00:25:07.227758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chkpt = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(chkpt)"
   ],
   "id": "7d34210d5801e28e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:10.320156Z",
     "start_time": "2026-02-08T00:25:09.800609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = load_dataset(\"csv\",data_files=\"data/train_cleaned.csv\")[\"train\"]\n",
    "val_dataset = load_dataset(\"csv\",data_files=\"data/val_cleaned.csv\")[\"train\"]\n",
    "categories = [\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]\n"
   ],
   "id": "d5e5891279d7c572",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:11.301661Z",
     "start_time": "2026-02-08T00:25:11.294616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_labels(batch):\n",
    "    label_lists = [batch[cat] for cat in categories]    \n",
    "    numpy_array = np.array(label_lists)\n",
    "    transpose = numpy_array.T\n",
    "    labels = transpose.tolist()\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "    }"
   ],
   "id": "34e597c76253a12e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:12.714244Z",
     "start_time": "2026-02-08T00:25:12.651617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.map(combine_labels, batched=True, batch_size=64)\n",
    "train_dataset"
   ],
   "id": "eed7fc663e37099e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance', 'word_count', 'labels'],\n",
       "    num_rows: 16771\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:14.857225Z",
     "start_time": "2026-02-08T00:25:14.848613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_dataset = val_dataset.map(combine_labels, batched=True, batch_size=64)\n",
    "val_dataset"
   ],
   "id": "6686c50a3f0d63e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance', 'labels'],\n",
       "    num_rows: 4201\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:16.377734Z",
     "start_time": "2026-02-08T00:25:16.345383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Sequence, Value\n",
    "\n",
    "new_features = train_dataset.features.copy()\n",
    "new_features[\"labels\"] = Sequence(Value(\"float32\"))\n",
    "train_dataset = train_dataset.cast(new_features)"
   ],
   "id": "9ed6aec8138af268",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:17.946223Z",
     "start_time": "2026-02-08T00:25:17.881503Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset[0]",
   "id": "84bd5928fd35c670",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Title: Rotation Invariance Neural Network Abstract: Rotation invariance and translation invariance have great values in image recognition tasks. In this paper, we bring a new architecture in convolutional neural network (CNN) named cyclic convolutional layer to achieve rotation invariance in 2-D symbol recognition. We can also get the position and orientation of the 2-D symbol by the network to achieve detection purpose for multiple non-overlap target. Last but not least, this architecture can achieve one-shot learning in some cases using those invariance.',\n",
       " 'Computer Science': 1,\n",
       " 'Physics': 0,\n",
       " 'Mathematics': 0,\n",
       " 'Statistics': 0,\n",
       " 'Quantitative Biology': 0,\n",
       " 'Quantitative Finance': 0,\n",
       " 'word_count': 82,\n",
       " 'labels': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:20.396927Z",
     "start_time": "2026-02-08T00:25:20.375825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_features = val_dataset.features.copy()\n",
    "new_features[\"labels\"] = Sequence(Value(\"float32\"))\n",
    "val_dataset = val_dataset.cast(new_features)"
   ],
   "id": "1fc935478bb42739",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:22.195730Z",
     "start_time": "2026-02-08T00:25:21.874886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_tokenized = train_dataset.map(\n",
    "    lambda batch: tokenizer(\n",
    "        batch[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "    max_length=512),batched=True, batch_size=64)\n",
    "print(train_tokenized.features)\n",
    "val_tokenized = val_dataset.map(\n",
    "    lambda batch: tokenizer(\n",
    "        batch[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "    max_length=512), batched=True, batch_size=64)\n",
    "print(val_tokenized.features)"
   ],
   "id": "d5790502f745140a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value('large_string'), 'Computer Science': Value('int64'), 'Physics': Value('int64'), 'Mathematics': Value('int64'), 'Statistics': Value('int64'), 'Quantitative Biology': Value('int64'), 'Quantitative Finance': Value('int64'), 'word_count': Value('int64'), 'labels': List(Value('float32')), 'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}\n",
      "{'text': Value('large_string'), 'Computer Science': Value('int64'), 'Physics': Value('int64'), 'Mathematics': Value('int64'), 'Statistics': Value('int64'), 'Quantitative Biology': Value('int64'), 'Quantitative Finance': Value('int64'), 'labels': List(Value('float32')), 'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T00:30:30.404469Z",
     "start_time": "2026-02-07T00:30:29.837332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "distill_bert_model = AutoModelForSequenceClassification.from_pretrained(chkpt)\n",
    "for encoder_layer, (name,param) in enumerate(distill_bert_model.named_parameters()):\n",
    "    print(encoder_layer,name)"
   ],
   "id": "a2a6a54c06437460",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e59b41f6d4a54e42b12cf213febdc811"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1mDistilBertForSequenceClassification LOAD REPORT\u001B[0m from: distilbert/distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001B[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 distilbert.embeddings.word_embeddings.weight\n",
      "1 distilbert.embeddings.position_embeddings.weight\n",
      "2 distilbert.embeddings.LayerNorm.weight\n",
      "3 distilbert.embeddings.LayerNorm.bias\n",
      "4 distilbert.transformer.layer.0.attention.q_lin.weight\n",
      "5 distilbert.transformer.layer.0.attention.q_lin.bias\n",
      "6 distilbert.transformer.layer.0.attention.k_lin.weight\n",
      "7 distilbert.transformer.layer.0.attention.k_lin.bias\n",
      "8 distilbert.transformer.layer.0.attention.v_lin.weight\n",
      "9 distilbert.transformer.layer.0.attention.v_lin.bias\n",
      "10 distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "11 distilbert.transformer.layer.0.attention.out_lin.bias\n",
      "12 distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "13 distilbert.transformer.layer.0.sa_layer_norm.bias\n",
      "14 distilbert.transformer.layer.0.ffn.lin1.weight\n",
      "15 distilbert.transformer.layer.0.ffn.lin1.bias\n",
      "16 distilbert.transformer.layer.0.ffn.lin2.weight\n",
      "17 distilbert.transformer.layer.0.ffn.lin2.bias\n",
      "18 distilbert.transformer.layer.0.output_layer_norm.weight\n",
      "19 distilbert.transformer.layer.0.output_layer_norm.bias\n",
      "20 distilbert.transformer.layer.1.attention.q_lin.weight\n",
      "21 distilbert.transformer.layer.1.attention.q_lin.bias\n",
      "22 distilbert.transformer.layer.1.attention.k_lin.weight\n",
      "23 distilbert.transformer.layer.1.attention.k_lin.bias\n",
      "24 distilbert.transformer.layer.1.attention.v_lin.weight\n",
      "25 distilbert.transformer.layer.1.attention.v_lin.bias\n",
      "26 distilbert.transformer.layer.1.attention.out_lin.weight\n",
      "27 distilbert.transformer.layer.1.attention.out_lin.bias\n",
      "28 distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "29 distilbert.transformer.layer.1.sa_layer_norm.bias\n",
      "30 distilbert.transformer.layer.1.ffn.lin1.weight\n",
      "31 distilbert.transformer.layer.1.ffn.lin1.bias\n",
      "32 distilbert.transformer.layer.1.ffn.lin2.weight\n",
      "33 distilbert.transformer.layer.1.ffn.lin2.bias\n",
      "34 distilbert.transformer.layer.1.output_layer_norm.weight\n",
      "35 distilbert.transformer.layer.1.output_layer_norm.bias\n",
      "36 distilbert.transformer.layer.2.attention.q_lin.weight\n",
      "37 distilbert.transformer.layer.2.attention.q_lin.bias\n",
      "38 distilbert.transformer.layer.2.attention.k_lin.weight\n",
      "39 distilbert.transformer.layer.2.attention.k_lin.bias\n",
      "40 distilbert.transformer.layer.2.attention.v_lin.weight\n",
      "41 distilbert.transformer.layer.2.attention.v_lin.bias\n",
      "42 distilbert.transformer.layer.2.attention.out_lin.weight\n",
      "43 distilbert.transformer.layer.2.attention.out_lin.bias\n",
      "44 distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "45 distilbert.transformer.layer.2.sa_layer_norm.bias\n",
      "46 distilbert.transformer.layer.2.ffn.lin1.weight\n",
      "47 distilbert.transformer.layer.2.ffn.lin1.bias\n",
      "48 distilbert.transformer.layer.2.ffn.lin2.weight\n",
      "49 distilbert.transformer.layer.2.ffn.lin2.bias\n",
      "50 distilbert.transformer.layer.2.output_layer_norm.weight\n",
      "51 distilbert.transformer.layer.2.output_layer_norm.bias\n",
      "52 distilbert.transformer.layer.3.attention.q_lin.weight\n",
      "53 distilbert.transformer.layer.3.attention.q_lin.bias\n",
      "54 distilbert.transformer.layer.3.attention.k_lin.weight\n",
      "55 distilbert.transformer.layer.3.attention.k_lin.bias\n",
      "56 distilbert.transformer.layer.3.attention.v_lin.weight\n",
      "57 distilbert.transformer.layer.3.attention.v_lin.bias\n",
      "58 distilbert.transformer.layer.3.attention.out_lin.weight\n",
      "59 distilbert.transformer.layer.3.attention.out_lin.bias\n",
      "60 distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "61 distilbert.transformer.layer.3.sa_layer_norm.bias\n",
      "62 distilbert.transformer.layer.3.ffn.lin1.weight\n",
      "63 distilbert.transformer.layer.3.ffn.lin1.bias\n",
      "64 distilbert.transformer.layer.3.ffn.lin2.weight\n",
      "65 distilbert.transformer.layer.3.ffn.lin2.bias\n",
      "66 distilbert.transformer.layer.3.output_layer_norm.weight\n",
      "67 distilbert.transformer.layer.3.output_layer_norm.bias\n",
      "68 distilbert.transformer.layer.4.attention.q_lin.weight\n",
      "69 distilbert.transformer.layer.4.attention.q_lin.bias\n",
      "70 distilbert.transformer.layer.4.attention.k_lin.weight\n",
      "71 distilbert.transformer.layer.4.attention.k_lin.bias\n",
      "72 distilbert.transformer.layer.4.attention.v_lin.weight\n",
      "73 distilbert.transformer.layer.4.attention.v_lin.bias\n",
      "74 distilbert.transformer.layer.4.attention.out_lin.weight\n",
      "75 distilbert.transformer.layer.4.attention.out_lin.bias\n",
      "76 distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "77 distilbert.transformer.layer.4.sa_layer_norm.bias\n",
      "78 distilbert.transformer.layer.4.ffn.lin1.weight\n",
      "79 distilbert.transformer.layer.4.ffn.lin1.bias\n",
      "80 distilbert.transformer.layer.4.ffn.lin2.weight\n",
      "81 distilbert.transformer.layer.4.ffn.lin2.bias\n",
      "82 distilbert.transformer.layer.4.output_layer_norm.weight\n",
      "83 distilbert.transformer.layer.4.output_layer_norm.bias\n",
      "84 distilbert.transformer.layer.5.attention.q_lin.weight\n",
      "85 distilbert.transformer.layer.5.attention.q_lin.bias\n",
      "86 distilbert.transformer.layer.5.attention.k_lin.weight\n",
      "87 distilbert.transformer.layer.5.attention.k_lin.bias\n",
      "88 distilbert.transformer.layer.5.attention.v_lin.weight\n",
      "89 distilbert.transformer.layer.5.attention.v_lin.bias\n",
      "90 distilbert.transformer.layer.5.attention.out_lin.weight\n",
      "91 distilbert.transformer.layer.5.attention.out_lin.bias\n",
      "92 distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "93 distilbert.transformer.layer.5.sa_layer_norm.bias\n",
      "94 distilbert.transformer.layer.5.ffn.lin1.weight\n",
      "95 distilbert.transformer.layer.5.ffn.lin1.bias\n",
      "96 distilbert.transformer.layer.5.ffn.lin2.weight\n",
      "97 distilbert.transformer.layer.5.ffn.lin2.bias\n",
      "98 distilbert.transformer.layer.5.output_layer_norm.weight\n",
      "99 distilbert.transformer.layer.5.output_layer_norm.bias\n",
      "100 pre_classifier.weight\n",
      "101 pre_classifier.bias\n",
      "102 classifier.weight\n",
      "103 classifier.bias\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:33.166506Z",
     "start_time": "2026-02-08T00:25:32.916214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"data/train_cleaned.csv\")\n",
    "total_samples = len(train_df)\n",
    "categories_distribution = train_df[categories].sum()\n",
    "categories_distribution"
   ],
   "id": "95eb212681d2c5f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computer Science        6875\n",
       "Physics                 4810\n",
       "Mathematics             4494\n",
       "Statistics              4165\n",
       "Quantitative Biology     470\n",
       "Quantitative Finance     199\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:35.319891Z",
     "start_time": "2026-02-08T00:25:35.307582Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_df)",
   "id": "2a688ad6851acda7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16771"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:36.949031Z",
     "start_time": "2026-02-08T00:25:36.936524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pos_weights = []\n",
    "for name, value in categories_distribution.items():\n",
    "    weight = (total_samples - value) / value\n",
    "    pos_weights.append(weight)\n",
    "pos_weights"
   ],
   "id": "d76225390ca342dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4394181818181817,\n",
       " 2.4866943866943867,\n",
       " 2.7318647085002223,\n",
       " 3.0266506602641057,\n",
       " 34.682978723404254,\n",
       " 83.27638190954774]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:39.078896Z",
     "start_time": "2026-02-08T00:25:39.068047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pos_weights_smoothed = np.sqrt(pos_weights)\n",
    "pos_weights_smoothed"
   ],
   "id": "1dc4680a372f1b38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19975755, 1.57692561, 1.65283535, 1.73972718, 5.88922565,\n",
       "       9.1255894 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:25:48.175972Z",
     "start_time": "2026-02-08T00:25:48.158519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pos_weights_tensor = torch.tensor(pos_weights_smoothed, dtype=torch.float32)\n",
    "def multi_label_loss_func(outputs,labels,num_items_in_batch):\n",
    "    logits = outputs.get(\"logits\")\n",
    "    loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights_tensor.to(logits.device))    \n",
    "    loss = loss_fct(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    logits = preds.predictions\n",
    "    labels = preds.label_ids\n",
    "    y_pred = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    f1_weighted = f1_score(y_true=labels, y_pred=y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_true=labels, y_pred=y_pred, average='macro')\n",
    "    precision_weighted = precision_score(y_true=labels, y_pred=y_pred, average='weighted')\n",
    "    precision_macro = precision_score(y_true=labels, y_pred=y_pred, average='macro')\n",
    "    recall_weighted = recall_score(y_true=labels, y_pred=y_pred, average='weighted')\n",
    "    recall_macro = recall_score(y_true=labels, y_pred=y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=y_pred, normalize=True)\n",
    "    h_loss = hamming_loss(labels, y_pred)\n",
    "    jaccard_macro = jaccard_score(labels, y_pred, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"recall_weighted\": recall_weighted,\n",
    "        \"accuracy_subset\": accuracy,\n",
    "        \"hamming_loss\": h_loss,\n",
    "        \"jaccard_macro\": jaccard_macro\n",
    "    }"
   ],
   "id": "8b8e36355fd192d8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:26:06.903029Z",
     "start_time": "2026-02-08T00:26:06.893415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "artificat_path = \"distill_bert_layer_4\"\n",
    "def tune_model(training_params):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(chkpt, num_labels=6, problem_type=\"multi_label_classification\").to(device)\n",
    "    for encoder_layer, (name,param) in enumerate(model.named_parameters()):\n",
    "        # if encoder_layer < 100: # only classification head\n",
    "        if encoder_layer < 84: # encoder layer 5\n",
    "        # if encoder_layer < 68: #encoder layer 4\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "                \n",
    "    training_args = TrainingArguments(\n",
    "        \"distil_bert_freeze\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate = training_params[\"learning_rate\"],\n",
    "        num_train_epochs=training_params[\"num_train_epochs\"],\n",
    "        remove_unused_columns=True,\n",
    "        weight_decay= training_params[\"weight_decay\"],\n",
    "        lr_scheduler_type=training_params[\"lr_scheduler\"],\n",
    "        warmup_ratio=training_params[\"warmup_ratio\"],\n",
    "        load_best_model_at_end=True,          \n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        greater_is_better=False,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        # disable_tqdm=True,\n",
    "        logging_steps=50,\n",
    "        gradient_accumulation_steps=8        \n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        args=training_args,\n",
    "        compute_loss_func=multi_label_loss_func,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    with mlflow.start_run(nested=True):     \n",
    "        print(f\"training_params ==== {training_params}\")\n",
    "        trainer.train()\n",
    "        model_tokenizer = {\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "        }\n",
    "        metrics = trainer.evaluate()\n",
    "        metrics[\"loss\"] = metrics[\"eval_loss\"]\n",
    "        metrics.pop(\"eval_loss\")        \n",
    "        print(f\"metrics ==== {metrics}\")\n",
    "        # mlflow.log_params(training_args)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.transformers.log_model(transformers_model=model_tokenizer, name=artificat_path, task=\"text-classification\")        \n",
    "        metrics[\"status\"] = STATUS_OK\n",
    "        return metrics"
   ],
   "id": "84acea13a755af1d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T00:26:09.690406Z",
     "start_time": "2026-02-08T00:26:09.679435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fine_tune_model(ml_flow_run_name, search_space):\n",
    "    with mlflow.start_run(run_name=ml_flow_run_name):\n",
    "        # Run optimization\n",
    "        trials = Trials()\n",
    "        best_params = fmin(\n",
    "            fn=tune_model,\n",
    "            space=search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=8,\n",
    "            trials=trials,\n",
    "            verbose=True,\n",
    "        )\n",
    "        # Find and log best results\n",
    "        best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n",
    "        # Log optimization results\n",
    "        mlflow.log_params(best_params)\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"best_val_log_loss\": best_trial[\"loss\"],\n",
    "                \"best_accuracy\": best_trial[\"accuracy\"],\n",
    "                \"best_precision\": best_trial[\"precision\"],\n",
    "                \"best_recall\": best_trial[\"recall\"],\n",
    "                \"best_f1\": best_trial[\"f1\"],\n",
    "                \"total_trials\": len(trials.trials),\n",
    "                \"optimization_completed\": 1,\n",
    "            }\n",
    "        )\n",
    "        print(f\"best_params ==== {best_params}\")"
   ],
   "id": "bd21bdbff92b839d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 3e-5, 5e-5),\n",
    "    \"num_train_epochs\": hp.uniformint(\"num_train_epochs\", 2, 3),\n",
    "    \"weight_decay\": hp.uniform(\"weight_decay\", 0.05, 0.1),\n",
    "    \"lr_scheduler\": hp.choice(\"lr_scheduler\", [\"cosine\", \"linear\"]),\n",
    "    \"warmup_ratio\": hp.choice(\"warmup_ratio\", [0.0, 0.05, 0.1]),\n",
    "}\n",
    "fine_tune_model(\"hyper_parameter_searchs\", search_space)"
   ],
   "id": "3e3e051a4fe80864",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
